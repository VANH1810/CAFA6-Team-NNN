{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation & Setup\n",
    "\n",
    "Installing required packages for comprehensive analysis:\n",
    "- **obonet**: Gene Ontology (GO) graph parsing\n",
    "- **biopython**: Protein sequence handling\n",
    "- **plotly**: Interactive visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T14:20:10.069152Z",
     "iopub.status.busy": "2025-12-14T14:20:10.068920Z",
     "iopub.status.idle": "2025-12-14T14:20:47.239277Z",
     "shell.execute_reply": "2025-12-14T14:20:47.238042Z",
     "shell.execute_reply.started": "2025-12-14T14:20:10.069131Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install obonet biopython -q\n",
    "!pip install --upgrade plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "Setting up our analysis toolkit with essential libraries for:\n",
    "- Data manipulation (pandas, numpy)\n",
    "- Visualization (matplotlib, seaborn, plotly)\n",
    "- Bioinformatics (Bio, obonet, networkx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T14:20:47.241657Z",
     "iopub.status.busy": "2025-12-14T14:20:47.241355Z",
     "iopub.status.idle": "2025-12-14T14:20:49.878732Z",
     "shell.execute_reply": "2025-12-14T14:20:49.877764Z",
     "shell.execute_reply.started": "2025-12-14T14:20:47.241632Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Core data science\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Bioinformatics\n",
    "from Bio import SeqIO\n",
    "import obonet\n",
    "import networkx as nx\n",
    "\n",
    "# Utilities\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Dict, List, Tuple\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"All libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Loading all competition files and performing initial validation checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T14:20:49.883519Z",
     "iopub.status.busy": "2025-12-14T14:20:49.883291Z",
     "iopub.status.idle": "2025-12-14T14:20:57.430239Z",
     "shell.execute_reply": "2025-12-14T14:20:57.429427Z",
     "shell.execute_reply.started": "2025-12-14T14:20:49.883501Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define paths\n",
    "BASE_PATH = '/kaggle/input/cafa-6-protein-function-prediction'\n",
    "TRAIN_PATH = f'{BASE_PATH}/Train'\n",
    "TEST_PATH = f'{BASE_PATH}/Test'\n",
    "\n",
    "paths = {\n",
    "    'train_seq': f'{TRAIN_PATH}/train_sequences.fasta',\n",
    "    'train_terms': f'{TRAIN_PATH}/train_terms.tsv',\n",
    "    'train_tax': f'{TRAIN_PATH}/train_taxonomy.tsv',\n",
    "    'go_obo': f'{TRAIN_PATH}/go-basic.obo',\n",
    "    'test_seq': f'{TEST_PATH}/testsuperset.fasta',\n",
    "    'ia': f'{BASE_PATH}/IA.tsv'\n",
    "}\n",
    "\n",
    "# Load tabular data\n",
    "print(\"Loading datasets...\")\n",
    "train_terms = pd.read_csv(paths['train_terms'], sep='\\t')\n",
    "train_tax = pd.read_csv(paths['train_tax'], sep='\\t', header=None, names=['EntryID', 'TaxonID'])\n",
    "ia_df = pd.read_csv(paths['ia'], sep='\\t', header=None, names=['term', 'IA'])\n",
    "\n",
    "# Load GO graph\n",
    "print(\"Loading Gene Ontology graph...\")\n",
    "go_graph = obonet.read_obo(paths['go_obo'])\n",
    "\n",
    "# Display data overview\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA OVERVIEW\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Training Terms:        {len(train_terms):,} annotations\")\n",
    "print(f\"Unique Proteins:       {train_terms['EntryID'].nunique():,}\")\n",
    "print(f\"Unique GO Terms:       {train_terms['term'].nunique():,}\")\n",
    "print(f\"Unique Species (Taxa): {train_tax['TaxonID'].nunique():,}\")\n",
    "print(f\"GO Graph Nodes:        {go_graph.number_of_nodes():,}\")\n",
    "print(f\"GO Graph Edges:        {go_graph.number_of_edges():,}\")\n",
    "print(f\"IA Records:            {len(ia_df):,}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Preview data\n",
    "display(train_terms.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); padding: 25px; border-radius: 12px; margin: 25px 0; box-shadow: 0 6px 15px rgba(0,0,0,0.2);\">\n",
    "  <h1 style=\"color: white; margin: 0; font-size: 32px; text-shadow: 2px 2px 4px rgba(0,0,0,0.3);\">\n",
    "    Protein Sequence Analysis\n",
    "  </h1>\n",
    "  <p style=\"color: rgba(255,255,255,0.9); margin: 10px 0 0 0; font-size: 16px;\">\n",
    "    Understanding the building blocks of protein function\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Loading and Basic Statistics\n",
    "\n",
    "Extracting key features from FASTA sequences:\n",
    "- Sequence lengths\n",
    "- Amino acid composition\n",
    "- Dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T14:20:57.431496Z",
     "iopub.status.busy": "2025-12-14T14:20:57.431226Z",
     "iopub.status.idle": "2025-12-14T14:21:01.276215Z",
     "shell.execute_reply": "2025-12-14T14:21:01.274994Z",
     "shell.execute_reply.started": "2025-12-14T14:20:57.431468Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_fasta_stats(fasta_path, max_seqs=None):\n",
    "    \"\"\"Load FASTA file and extract sequence statistics\"\"\"\n",
    "    data = []\n",
    "    aa_counter = Counter()\n",
    "    \n",
    "    for i, record in enumerate(tqdm(SeqIO.parse(fasta_path, 'fasta'), desc=\"Loading sequences\")):\n",
    "        if max_seqs and i >= max_seqs:\n",
    "            break\n",
    "            \n",
    "        entry_id = record.id.split('|')[1] if '|' in record.id else record.id\n",
    "        seq = str(record.seq)\n",
    "        \n",
    "        data.append({\n",
    "            'EntryID': entry_id,\n",
    "            'length': len(seq),\n",
    "            'seq': seq\n",
    "        })\n",
    "        aa_counter.update(seq)\n",
    "    \n",
    "    return pd.DataFrame(data), aa_counter\n",
    "\n",
    "# Load training sequences\n",
    "print(\"Loading training sequences...\")\n",
    "train_seqs, train_aa_counts = load_fasta_stats(paths['train_seq'])\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nSequence Statistics:\")\n",
    "print(f\"   Total sequences: {len(train_seqs):,}\")\n",
    "print(f\"   Mean length: {train_seqs['length'].mean():.1f} amino acids\")\n",
    "print(f\"   Median length: {train_seqs['length'].median():.0f} amino acids\")\n",
    "print(f\"   Min length: {train_seqs['length'].min()}\")\n",
    "print(f\"   Max length: {train_seqs['length'].max():,}\")\n",
    "print(f\"   Std dev: {train_seqs['length'].std():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Sequence Length Distribution\n",
    "\n",
    "Understanding the distribution of protein lengths helps with:\n",
    "- Batch size selection for training\n",
    "- Model architecture decisions (e.g., max sequence length)\n",
    "- Identifying potential outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T14:21:01.277632Z",
     "iopub.status.busy": "2025-12-14T14:21:01.277307Z",
     "iopub.status.idle": "2025-12-14T14:21:03.443600Z",
     "shell.execute_reply": "2025-12-14T14:21:03.441794Z",
     "shell.execute_reply.started": "2025-12-14T14:21:01.277605Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create comprehensive length visualization\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Distribution', 'Box Plot', 'Cumulative Distribution', 'Log Scale'),\n",
    "    specs=[[{\"type\": \"histogram\"}, {\"type\": \"box\"}],\n",
    "           [{\"type\": \"scatter\"}, {\"type\": \"histogram\"}]]\n",
    ")\n",
    "\n",
    "# Histogram\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=train_seqs['length'], nbinsx=100, name='Count',\n",
    "                 marker_color='lightblue', showlegend=False),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Box plot\n",
    "fig.add_trace(\n",
    "    go.Box(y=train_seqs['length'], name='Length', marker_color='lightcoral',\n",
    "           boxmean='sd', showlegend=False),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Cumulative distribution\n",
    "sorted_lengths = np.sort(train_seqs['length'])\n",
    "cumulative = np.arange(1, len(sorted_lengths) + 1) / len(sorted_lengths) * 100\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=sorted_lengths, y=cumulative, mode='lines',\n",
    "               name='Cumulative %', line=dict(color='green', width=2),\n",
    "               showlegend=False),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Log scale histogram\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=train_seqs['length'], nbinsx=100, name='Count (log)',\n",
    "                 marker_color='mediumpurple', showlegend=False),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Sequence Length\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"Sequence Length (log)\", type=\"log\", row=2, col=2)\n",
    "fig.update_yaxes(title_text=\"Count\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Cumulative %\", row=2, col=1)\n",
    "\n",
    "fig.update_layout(height=800, title_text=\"Protein Sequence Length Analysis\", showlegend=False)\n",
    "fig.show()\n",
    "\n",
    "# Key percentiles\n",
    "percentiles = train_seqs['length'].quantile([0.25, 0.5, 0.75, 0.90, 0.95, 0.99])\n",
    "print(\"\\nKey Percentiles:\")\n",
    "for p, val in percentiles.items():\n",
    "    print(f\"   {int(p*100)}th percentile: {val:.0f} amino acids\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Amino Acid Composition Analysis\n",
    "\n",
    "The 20 standard amino acids and their properties:\n",
    "- **Hydrophobic**: A, V, I, L, M, F, W, P\n",
    "- **Polar**: S, T, C, Y, N, Q\n",
    "- **Charged (+)**: K, R, H\n",
    "- **Charged (-)**: D, E\n",
    "- **Special**: G (smallest)\n",
    "\n",
    "Understanding amino acid distribution can reveal:\n",
    "- Dataset biases\n",
    "- Protein type composition\n",
    "- Potential feature engineering opportunities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T14:21:03.447829Z",
     "iopub.status.busy": "2025-12-14T14:21:03.447364Z",
     "iopub.status.idle": "2025-12-14T14:21:03.516963Z",
     "shell.execute_reply": "2025-12-14T14:21:03.515906Z",
     "shell.execute_reply.started": "2025-12-14T14:21:03.447790Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Calculate amino acid frequencies\n",
    "aa_total = sum(train_aa_counts.values())\n",
    "aa_freq = {aa: count/aa_total*100 for aa, count in train_aa_counts.items()}\n",
    "\n",
    "# Sort by frequency\n",
    "aa_sorted = sorted(aa_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "amino_acids = [x[0] for x in aa_sorted]\n",
    "frequencies = [x[1] for x in aa_sorted]\n",
    "\n",
    "# Amino acid properties\n",
    "aa_properties = {\n",
    "    'L': 'Hydrophobic', 'A': 'Hydrophobic', 'G': 'Special', 'V': 'Hydrophobic',\n",
    "    'S': 'Polar', 'E': 'Charged(-)', 'I': 'Hydrophobic', 'K': 'Charged(+)',\n",
    "    'R': 'Charged(+)', 'T': 'Polar', 'D': 'Charged(-)', 'P': 'Hydrophobic',\n",
    "    'N': 'Polar', 'Q': 'Polar', 'F': 'Hydrophobic', 'Y': 'Polar',\n",
    "    'M': 'Hydrophobic', 'H': 'Charged(+)', 'C': 'Polar', 'W': 'Hydrophobic'\n",
    "}\n",
    "\n",
    "colors = {\n",
    "    'Hydrophobic': '#3498db',\n",
    "    'Polar': '#2ecc71',\n",
    "    'Charged(+)': '#e74c3c',\n",
    "    'Charged(-)': '#f39c12',\n",
    "    'Special': '#9b59b6'\n",
    "}\n",
    "\n",
    "bar_colors = [colors[aa_properties.get(aa, 'Special')] for aa in amino_acids]\n",
    "\n",
    "# Create visualization\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=amino_acids,\n",
    "    y=frequencies,\n",
    "    marker_color=bar_colors,\n",
    "    text=[f'{f:.2f}%' for f in frequencies],\n",
    "    textposition='outside',\n",
    "    hovertemplate='<b>%{x}</b><br>Frequency: %{y:.3f}%<br>Property: ' + \n",
    "                  '<extra></extra>'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Amino Acid Composition in Training Proteins',\n",
    "    xaxis_title='Amino Acid',\n",
    "    yaxis_title='Frequency (%)',\n",
    "    height=500,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Group by property\n",
    "property_freq = defaultdict(float)\n",
    "for aa, freq in aa_freq.items():\n",
    "    prop = aa_properties.get(aa, 'Special')\n",
    "    property_freq[prop] += freq\n",
    "\n",
    "print(\"\\nAmino Acid Composition by Property:\")\n",
    "for prop, freq in sorted(property_freq.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"   {prop:15s}: {freq:5.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%); padding: 25px; border-radius: 12px; margin: 25px 0; box-shadow: 0 6px 15px rgba(0,0,0,0.2);\">\n",
    "  <h1 style=\"color: white; margin: 0; font-size: 32px; text-shadow: 2px 2px 4px rgba(0,0,0,0.3);\">\n",
    "    GO Terms & Label Analysis\n",
    "  </h1>\n",
    "  <p style=\"color: rgba(255,255,255,0.9); margin: 10px 0 0 0; font-size: 16px;\">\n",
    "    Understanding the prediction targets and class distribution\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Class Imbalance & Distribution\n",
    "\n",
    "The CAFA challenge involves predicting GO terms across three subontologies:\n",
    "- **MFO**: Molecular Function (What does the protein do?)\n",
    "- **BPO**: Biological Process (What pathway is it involved in?)\n",
    "- **CCO**: Cellular Component (Where is it located?)\n",
    "\n",
    "Understanding class imbalance is crucial for:\n",
    "- Choosing appropriate loss functions\n",
    "- Setting up balanced validation sets\n",
    "- Calibrating prediction thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T14:21:03.518502Z",
     "iopub.status.busy": "2025-12-14T14:21:03.518137Z",
     "iopub.status.idle": "2025-12-14T14:21:03.952623Z",
     "shell.execute_reply": "2025-12-14T14:21:03.951618Z",
     "shell.execute_reply.started": "2025-12-14T14:21:03.518478Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Map aspect codes\n",
    "aspect_map = {'F': 'MFO', 'P': 'BPO', 'C': 'CCO'}\n",
    "train_terms['aspect_name'] = train_terms['aspect'].map(aspect_map)\n",
    "\n",
    "# Overall statistics\n",
    "print(\"=\"*70)\n",
    "print(\"LABEL DISTRIBUTION STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total annotations:        {len(train_terms):,}\")\n",
    "print(f\"Unique proteins:          {train_terms['EntryID'].nunique():,}\")\n",
    "print(f\"Unique GO terms:          {train_terms['term'].nunique():,}\")\n",
    "print(f\"Avg terms per protein:    {len(train_terms) / train_terms['EntryID'].nunique():.2f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Aspect distribution\n",
    "aspect_counts = train_terms['aspect_name'].value_counts()\n",
    "print(\"\\nDistribution by Subontology:\")\n",
    "for aspect, count in aspect_counts.items():\n",
    "    pct = count / len(train_terms) * 100\n",
    "    print(f\"   {aspect}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Visualize aspect distribution\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=('Annotation Count', 'Unique Terms'),\n",
    "    specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}]]\n",
    ")\n",
    "\n",
    "# Annotation counts\n",
    "fig.add_trace(\n",
    "    go.Bar(x=aspect_counts.index, y=aspect_counts.values,\n",
    "           marker_color=['#3498db', '#e74c3c', '#2ecc71'],\n",
    "           text=aspect_counts.values, textposition='outside'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Unique terms per aspect\n",
    "unique_terms = train_terms.groupby('aspect_name')['term'].nunique()\n",
    "fig.add_trace(\n",
    "    go.Bar(x=unique_terms.index, y=unique_terms.values,\n",
    "           marker_color=['#3498db', '#e74c3c', '#2ecc71'],\n",
    "           text=unique_terms.values, textposition='outside'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=400, showlegend=False,\n",
    "                 title_text='GO Term Distribution by Subontology')\n",
    "fig.update_xaxes(title_text=\"Subontology\")\n",
    "fig.update_yaxes(title_text=\"Count\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Unique Terms\", row=1, col=2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Term Frequency Analysis\n",
    "\n",
    "**The Long-Tail Problem:**\n",
    "\n",
    "CAFA exhibits extreme class imbalance - a few terms appear very frequently,\n",
    "while most terms are rare. This is the classic \"long-tail distribution\" that\n",
    "makes this challenge particularly difficult.\n",
    "\n",
    "**Key Statistics:**\n",
    "- Most frequent term vs least frequent term ratio\n",
    "- Percentage of terms appearing < 10 times\n",
    "- Head vs tail contribution to total annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T14:21:03.953963Z",
     "iopub.status.busy": "2025-12-14T14:21:03.953624Z",
     "iopub.status.idle": "2025-12-14T14:21:04.090708Z",
     "shell.execute_reply": "2025-12-14T14:21:04.089504Z",
     "shell.execute_reply.started": "2025-12-14T14:21:03.953939Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Calculate term frequencies\n",
    "term_freq = train_terms['term'].value_counts()\n",
    "\n",
    "# Statistics\n",
    "print(\"=\"*70)\n",
    "print(\"TERM FREQUENCY STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total unique terms:       {len(term_freq):,}\")\n",
    "print(f\"Most frequent term:       {term_freq.iloc[0]:,} proteins\")\n",
    "print(f\"Least frequent term:      {term_freq.iloc[-1]:,} proteins\")\n",
    "print(f\"Median frequency:         {term_freq.median():.0f} proteins\")\n",
    "print(f\"Mean frequency:           {term_freq.mean():.1f} proteins\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Rare terms analysis\n",
    "rare_terms = (term_freq <= 5).sum()\n",
    "very_rare = (term_freq == 1).sum()\n",
    "print(f\"\\nRare Terms Analysis:\")\n",
    "print(f\"   Terms appearing ≤5 times:  {rare_terms:,} ({rare_terms/len(term_freq)*100:.1f}%)\")\n",
    "print(f\"   Terms appearing once:      {very_rare:,} ({very_rare/len(term_freq)*100:.1f}%)\")\n",
    "\n",
    "# Head vs tail\n",
    "head_20_pct = term_freq.iloc[:int(len(term_freq)*0.2)].sum()\n",
    "tail_80_pct = term_freq.iloc[int(len(term_freq)*0.2):].sum()\n",
    "print(f\"\\nPareto Analysis:\")\n",
    "print(f\"   Top 20% terms cover:       {head_20_pct/term_freq.sum()*100:.1f}% of annotations\")\n",
    "print(f\"   Bottom 80% terms cover:    {tail_80_pct/term_freq.sum()*100:.1f}% of annotations\")\n",
    "\n",
    "# Visualization: Log-scale distribution\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=np.arange(1, len(term_freq) + 1),\n",
    "    y=term_freq.values,\n",
    "    mode='lines',\n",
    "    name='Term Frequency',\n",
    "    line=dict(color='#3498db', width=2),\n",
    "    fill='tozeroy'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Term Frequency Distribution (Long-Tail)',\n",
    "    xaxis_title='Term Rank',\n",
    "    yaxis_title='Frequency (log scale)',\n",
    "    yaxis_type='log',\n",
    "    height=500,\n",
    "    hovermode='x'\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Top 20 most frequent terms\n",
    "print(\"\\nTop 20 Most Frequent Terms:\")\n",
    "top_20 = term_freq.head(20)\n",
    "for i, (term, count) in enumerate(top_20.items(), 1):\n",
    "    term_name = go_graph.nodes.get(term, {}).get('name', 'Unknown')\n",
    "    print(f\"   {i:2d}. {term}: {count:5,} | {term_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Information Accretion (IA) Weights\n",
    "\n",
    "**What is IA?**\n",
    "\n",
    "Information Accretion measures how \"informative\" a GO term is:\n",
    "- **High IA**: Rare, specific terms (hard to predict, high value)\n",
    "- **Low IA**: Common, general terms (easier to predict, lower value)\n",
    "\n",
    "The competition metric (F-max) is weighted by IA, meaning:\n",
    "✅ Correctly predicting rare terms is rewarded more\n",
    "❌ Missing rare terms is penalized more\n",
    "\n",
    "**Key Question**: Is there a relationship between term frequency and IA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T14:21:04.092367Z",
     "iopub.status.busy": "2025-12-14T14:21:04.091978Z",
     "iopub.status.idle": "2025-12-14T14:21:04.281380Z",
     "shell.execute_reply": "2025-12-14T14:21:04.279657Z",
     "shell.execute_reply.started": "2025-12-14T14:21:04.092335Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Merge frequency and IA\n",
    "term_freq_df = term_freq.reset_index()\n",
    "term_freq_df.columns = ['term', 'frequency']\n",
    "merged = term_freq_df.merge(ia_df, on='term', how='left')\n",
    "\n",
    "# Statistics\n",
    "print(\"=\"*70)\n",
    "print(\"INFORMATION ACCRETION (IA) STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Terms with IA values:     {merged['IA'].notna().sum():,}\")\n",
    "print(f\"Mean IA:                  {ia_df['IA'].mean():.3f}\")\n",
    "print(f\"Median IA:                {ia_df['IA'].median():.3f}\")\n",
    "print(f\"Max IA:                   {ia_df['IA'].max():.3f}\")\n",
    "print(f\"Min IA:                   {ia_df['IA'].min():.3f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Correlation analysis\n",
    "corr_pearson = merged[['frequency', 'IA']].corr().iloc[0, 1]\n",
    "corr_spearman = merged[['frequency', 'IA']].corr(method='spearman').iloc[0, 1]\n",
    "\n",
    "print(f\"\\nFrequency vs IA Correlation:\")\n",
    "print(f\"   Pearson:  {corr_pearson:.4f}\")\n",
    "print(f\"   Spearman: {corr_spearman:.4f}\")\n",
    "print(f\"\\nNegative correlation confirms: Rare terms have higher IA!\")\n",
    "\n",
    "# Visualization\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=('Frequency vs IA (log-scale)', 'IA Distribution'),\n",
    ")\n",
    "\n",
    "# Scatter plot\n",
    "sample = merged.sample(min(10000, len(merged)), random_state=42)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=sample['frequency'], y=sample['IA'],\n",
    "        mode='markers',\n",
    "        marker=dict(size=5, color=sample['IA'], colorscale='Viridis',\n",
    "                   showscale=True, colorbar=dict(title=\"IA\")),\n",
    "        text=sample['term'],\n",
    "        hovertemplate='<b>%{text}</b><br>Freq: %{x}<br>IA: %{y:.3f}<extra></extra>'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# IA distribution\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=ia_df['IA'], nbinsx=50, marker_color='lightcoral'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Frequency (log)\", type=\"log\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"IA Value\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"IA\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Count\", row=1, col=2)\n",
    "fig.update_layout(height=500, showlegend=False,\n",
    "                 title_text='Information Accretion Analysis')\n",
    "fig.show()\n",
    "\n",
    "# High-value targets\n",
    "high_ia = merged.nlargest(15, 'IA')\n",
    "print(\"\\nTop 15 Highest-Value Terms (High IA):\")\n",
    "for i, row in enumerate(high_ia.itertuples(), 1):\n",
    "    term_name = go_graph.nodes.get(row.term, {}).get('name', 'Unknown')\n",
    "    print(f\"   {i:2d}. IA={row.IA:.3f} | Freq={row.frequency:3d} | {term_name[:50]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Label Cardinality (Multi-Label Nature)\n",
    "\n",
    "**Multi-Label Classification:**\n",
    "\n",
    "Unlike traditional classification where each sample has ONE label, in CAFA:\n",
    "- Each protein can have MULTIPLE GO terms\n",
    "- Terms can be from different subontologies\n",
    "- This affects model architecture and evaluation\n",
    "\n",
    "**Questions to explore:**\n",
    "- How many labels does a typical protein have?\n",
    "- Are there proteins with very few or very many labels?\n",
    "- How does label count vary by subontology?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T14:21:04.283300Z",
     "iopub.status.busy": "2025-12-14T14:21:04.282843Z",
     "iopub.status.idle": "2025-12-14T14:21:05.038364Z",
     "shell.execute_reply": "2025-12-14T14:21:05.037496Z",
     "shell.execute_reply.started": "2025-12-14T14:21:04.283266Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Calculate labels per protein\n",
    "labels_per_protein = train_terms.groupby('EntryID')['term'].count()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LABEL CARDINALITY STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Mean labels per protein:  {labels_per_protein.mean():.2f}\")\n",
    "print(f\"Median labels per protein:{labels_per_protein.median():.0f}\")\n",
    "print(f\"Min labels:               {labels_per_protein.min()}\")\n",
    "print(f\"Max labels:               {labels_per_protein.max()}\")\n",
    "print(f\"Std deviation:            {labels_per_protein.std():.2f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Percentiles\n",
    "percentiles = labels_per_protein.quantile([0.25, 0.5, 0.75, 0.90, 0.95, 0.99])\n",
    "print(\"\\nLabel Count Percentiles:\")\n",
    "for p, val in percentiles.items():\n",
    "    print(f\"   {int(p*100)}th percentile: {val:.0f} labels\")\n",
    "\n",
    "# Visualization\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Overall Distribution', 'By Subontology', \n",
    "                   'Cumulative Distribution', 'Violin Plot by Aspect'),\n",
    "    specs=[[{\"type\": \"histogram\"}, {\"type\": \"bar\"}],\n",
    "           [{\"type\": \"scatter\"}, {\"type\": \"violin\"}]]\n",
    ")\n",
    "\n",
    "# Overall histogram\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=labels_per_protein, nbinsx=50, marker_color='lightblue'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# By subontology\n",
    "labels_by_aspect = train_terms.groupby(['EntryID', 'aspect_name'])['term'].count().reset_index()\n",
    "avg_by_aspect = labels_by_aspect.groupby('aspect_name')['term'].mean().sort_values(ascending=False)\n",
    "fig.add_trace(\n",
    "    go.Bar(x=avg_by_aspect.index, y=avg_by_aspect.values,\n",
    "           marker_color=['#3498db', '#e74c3c', '#2ecc71'],\n",
    "           text=[f'{v:.2f}' for v in avg_by_aspect.values],\n",
    "           textposition='outside'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Cumulative distribution\n",
    "sorted_labels = np.sort(labels_per_protein)\n",
    "cumulative = np.arange(1, len(sorted_labels) + 1) / len(sorted_labels) * 100\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=sorted_labels, y=cumulative, mode='lines',\n",
    "               line=dict(color='green', width=2)),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Violin plot by aspect\n",
    "for aspect in ['BPO', 'CCO', 'MFO']:\n",
    "    aspect_data = labels_by_aspect[labels_by_aspect['aspect_name'] == aspect]['term']\n",
    "    fig.add_trace(\n",
    "        go.Violin(y=aspect_data, name=aspect, box_visible=True, meanline_visible=True),\n",
    "        row=2, col=2\n",
    "    )\n",
    "\n",
    "fig.update_xaxes(title_text=\"Number of Labels\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Subontology\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Number of Labels\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Count\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Avg Labels\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Cumulative %\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Label Count\", row=2, col=2)\n",
    "\n",
    "fig.update_layout(height=800, showlegend=False,\n",
    "                 title_text='Multi-Label Cardinality Analysis')\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(f\"   Most proteins have {labels_per_protein.median():.0f}-{labels_per_protein.quantile(0.75):.0f} labels\")\n",
    "print(f\"   {(labels_per_protein == 1).sum():,} proteins have only 1 label\")\n",
    "print(f\"   {(labels_per_protein > 20).sum():,} proteins have >20 labels (complex functions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 25px; border-radius: 12px; margin: 25px 0; box-shadow: 0 6px 15px rgba(0,0,0,0.2);\">\n",
    "  <h1 style=\"color: white; margin: 0; font-size: 32px; text-shadow: 2px 2px 4px rgba(0,0,0,0.3);\">\n",
    "    Gene Ontology Hierarchy\n",
    "  </h1>\n",
    "  <p style=\"color: rgba(255,255,255,0.9); margin: 10px 0 0 0; font-size: 16px;\">\n",
    "    Exploring the DAG structure and term relationships\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gene Ontology is organized as a **Directed Acyclic Graph (DAG)**:\n",
    "- **Directed**: Parent → Child relationships (general → specific)\n",
    "- **Acyclic**: No circular dependencies\n",
    "- **Multiple inheritance**: A term can have multiple parents\n",
    "\n",
    "**Key relationships:**\n",
    "- `is_a`: Child is a type of parent (e.g., \"ATP binding\" is_a \"nucleotide binding\")\n",
    "- `part_of`: Child is part of parent process\n",
    "\n",
    "**True Path Rule**: If a protein has term X, it also has all ancestors of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T14:21:05.039582Z",
     "iopub.status.busy": "2025-12-14T14:21:05.039321Z",
     "iopub.status.idle": "2025-12-14T14:21:05.418305Z",
     "shell.execute_reply": "2025-12-14T14:21:05.417273Z",
     "shell.execute_reply.started": "2025-12-14T14:21:05.039544Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define ontology roots\n",
    "roots = {\n",
    "    'BPO': 'GO:0008150',  # biological_process\n",
    "    'CCO': 'GO:0005575',  # cellular_component\n",
    "    'MFO': 'GO:0003674'   # molecular_function\n",
    "}\n",
    "\n",
    "# Basic graph statistics\n",
    "print(\"=\"*70)\n",
    "print(\"GENE ONTOLOGY GRAPH STRUCTURE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total GO terms (nodes):   {go_graph.number_of_nodes():,}\")\n",
    "print(f\"Total relationships:      {go_graph.number_of_edges():,}\")\n",
    "print(f\"Avg edges per node:       {go_graph.number_of_edges()/go_graph.number_of_nodes():.2f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Analyze each subontology\n",
    "print(\"\\nSubontology Statistics:\")\n",
    "for name, root in roots.items():\n",
    "    if root in go_graph:\n",
    "        # Get all descendants\n",
    "        descendants = nx.descendants(go_graph, root)\n",
    "        descendants.add(root)\n",
    "        \n",
    "        # Count terms in training set\n",
    "        aspect_code = {'BPO': 'P', 'CCO': 'C', 'MFO': 'F'}[name]\n",
    "        terms_in_train = train_terms[train_terms['aspect'] == aspect_code]['term'].nunique()\n",
    "        \n",
    "        print(f\"\\n   {name}:\")\n",
    "        print(f\"      Total terms in GO:     {len(descendants):,}\")\n",
    "        print(f\"      Used in training:      {terms_in_train:,} ({terms_in_train/len(descendants)*100:.1f}%)\")\n",
    "\n",
    "# Sample term exploration\n",
    "sample_term = 'GO:0005515'  # protein binding\n",
    "if sample_term in go_graph:\n",
    "    term_info = go_graph.nodes[sample_term]\n",
    "    print(f\"\\nExample Term: {sample_term}\")\n",
    "    print(f\"   Name: {term_info.get('name', 'N/A')}\")\n",
    "    print(f\"   Namespace: {term_info.get('namespace', 'N/A')}\")\n",
    "    print(f\"   Definition: {term_info.get('def', 'N/A')[:100]}...\")\n",
    "    \n",
    "    # Parents and children\n",
    "    parents = list(go_graph.predecessors(sample_term))\n",
    "    children = list(go_graph.successors(sample_term))\n",
    "    print(f\"   Parent terms: {len(parents)}\")\n",
    "    print(f\"   Child terms: {len(children)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%); padding: 25px; border-radius: 12px; margin: 25px 0; box-shadow: 0 6px 15px rgba(0,0,0,0.2);\">\n",
    "  <h1 style=\"color: #334155; margin: 0; font-size: 32px; text-shadow: 1px 1px 2px rgba(255,255,255,0.5);\">\n",
    "    Term Depth Analysis\n",
    "  </h1>\n",
    "  <p style=\"color: #475569; margin: 10px 0 0 0; font-size: 16px;\">\n",
    "    Understanding hierarchy depth and IA relationships\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Depth** = shortest path from root to term\n",
    "\n",
    "**Why depth matters:**\n",
    "- Shallow terms (depth 1-3): General, common functions\n",
    "- Medium depth (4-7): Specific functional categories  \n",
    "- Deep terms (8+): Highly specific, rare functions\n",
    "\n",
    "**Hypothesis**: Deeper terms should have higher IA (more informative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T14:21:05.419696Z",
     "iopub.status.busy": "2025-12-14T14:21:05.419414Z",
     "iopub.status.idle": "2025-12-14T14:21:36.475061Z",
     "shell.execute_reply": "2025-12-14T14:21:36.474062Z",
     "shell.execute_reply.started": "2025-12-14T14:21:05.419674Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_depths(graph, root):\n",
    "    \"\"\"Calculate shortest path from root to all descendants\"\"\"\n",
    "    depths = {}\n",
    "    if root not in graph:\n",
    "        return depths\n",
    "    \n",
    "    depths[root] = 0\n",
    "    \n",
    "    # Get all nodes that can reach the root\n",
    "    for node in graph.nodes():\n",
    "        if node == root:\n",
    "            continue\n",
    "        try:\n",
    "            # In obonet, edges point FROM child TO parent\n",
    "            depth = nx.shortest_path_length(graph, node, root)\n",
    "            depths[node] = depth\n",
    "        except nx.NetworkXNoPath:\n",
    "            pass\n",
    "    \n",
    "    return depths\n",
    "\n",
    "# Calculate depths for all subontologies\n",
    "all_depths = {}\n",
    "for name, root in roots.items():\n",
    "    depths = calculate_depths(go_graph, root)\n",
    "    all_depths[name] = depths\n",
    "    \n",
    "    if depths:\n",
    "        print(f\"\\n{name} Depth Statistics:\")\n",
    "        depth_values = list(depths.values())\n",
    "        print(f\"   Max depth:     {max(depth_values)}\")\n",
    "        print(f\"   Mean depth:    {np.mean(depth_values):.2f}\")\n",
    "        print(f\"   Median depth:  {np.median(depth_values):.0f}\")\n",
    "\n",
    "# Create depth dataframe for training terms\n",
    "depth_data = []\n",
    "for _, row in train_terms.iterrows():\n",
    "    term = row['term']\n",
    "    aspect = row['aspect_name']\n",
    "    \n",
    "    if aspect in all_depths and term in all_depths[aspect]:\n",
    "        depth = all_depths[aspect][term]\n",
    "        depth_data.append({\n",
    "            'term': term,\n",
    "            'aspect': aspect,\n",
    "            'depth': depth\n",
    "        })\n",
    "\n",
    "depth_df = pd.DataFrame(depth_data)\n",
    "\n",
    "# Check if we have data\n",
    "if len(depth_df) == 0:\n",
    "    print(\"\\nWarning: No depth data calculated. Check GO graph structure.\")\n",
    "else:\n",
    "    print(f\"\\nCalculated depths for {len(depth_df):,} terms\")\n",
    "\n",
    "# Merge with IA (only if we have depth data)\n",
    "if len(depth_df) > 0:\n",
    "    depth_ia = depth_df.merge(ia_df, on='term', how='left')\n",
    "    \n",
    "    # Visualization\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=('Depth Distribution by Subontology', 'Depth vs IA Relationship')\n",
    "    )\n",
    "    \n",
    "    # Depth distribution\n",
    "    for aspect in ['BPO', 'CCO', 'MFO']:\n",
    "        aspect_depths = depth_df[depth_df['aspect'] == aspect]['depth']\n",
    "        if len(aspect_depths) > 0:\n",
    "            fig.add_trace(\n",
    "                go.Violin(y=aspect_depths, name=aspect, box_visible=True, meanline_visible=True),\n",
    "                row=1, col=1\n",
    "            )\n",
    "    \n",
    "    # Depth vs IA scatter\n",
    "    depth_ia_clean = depth_ia.dropna()\n",
    "    if len(depth_ia_clean) > 0:\n",
    "        sample = depth_ia_clean.sample(min(5000, len(depth_ia_clean)), random_state=42)\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=sample['depth'], y=sample['IA'],\n",
    "                mode='markers',\n",
    "                marker=dict(size=4, color=sample['IA'], colorscale='Viridis',\n",
    "                           opacity=0.6),\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # Add trend line\n",
    "        from scipy import stats\n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(\n",
    "            depth_ia_clean['depth'], depth_ia_clean['IA']\n",
    "        )\n",
    "        line_x = np.array([depth_ia_clean['depth'].min(), depth_ia_clean['depth'].max()])\n",
    "        line_y = slope * line_x + intercept\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=line_x, y=line_y, mode='lines',\n",
    "                       line=dict(color='red', width=2, dash='dash'),\n",
    "                       name=f'Trend (R²={r_value**2:.3f})'),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nDepth-IA Correlation: {r_value:.3f}\")\n",
    "        print(f\"   Positive correlation confirms: Deeper terms have higher IA!\")\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Subontology\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Depth\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Depth\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"IA\", row=1, col=2)\n",
    "    fig.update_layout(height=500, title_text='GO Term Depth Analysis')\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"Skipping visualization due to missing depth data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(135deg, #43e97b 0%, #38f9d7 100%); padding: 25px; border-radius: 12px; margin: 25px 0; box-shadow: 0 6px 15px rgba(0,0,0,0.2);\">\n",
    "  <h1 style=\"color: white; margin: 0; font-size: 32px; text-shadow: 2px 2px 4px rgba(0,0,0,0.3);\">\n",
    "    Taxonomy Analysis\n",
    "  </h1>\n",
    "  <p style=\"color: rgba(255,255,255,0.9); margin: 10px 0 0 0; font-size: 16px;\">\n",
    "    Understanding species distribution in training data\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Why taxonomy matters:**\n",
    "- Proteins from related species often have similar functions\n",
    "- Evolutionary conservation can be leveraged for prediction\n",
    "- Understanding species distribution helps assess model generalization\n",
    "\n",
    "**Model organisms** (well-studied species) dominate the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T14:21:36.476501Z",
     "iopub.status.busy": "2025-12-14T14:21:36.476162Z",
     "iopub.status.idle": "2025-12-14T14:21:36.547380Z",
     "shell.execute_reply": "2025-12-14T14:21:36.546288Z",
     "shell.execute_reply.started": "2025-12-14T14:21:36.476469Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Species frequency\n",
    "species_counts = train_tax['TaxonID'].value_counts()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TAXONOMY STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total proteins:           {len(train_tax):,}\")\n",
    "print(f\"Unique species (taxa):    {len(species_counts):,}\")\n",
    "print(f\"Most common taxon:        {species_counts.iloc[0]:,} proteins\")\n",
    "print(f\"Median proteins per taxon:{species_counts.median():.0f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Top species (common model organisms)\n",
    "print(\"\\nTop 15 Species by Protein Count:\")\n",
    "top_species = species_counts.head(15)\n",
    "\n",
    "# Common model organism IDs\n",
    "model_organisms = {\n",
    "    9606: 'Homo sapiens (Human)',\n",
    "    10090: 'Mus musculus (Mouse)',\n",
    "    10116: 'Rattus norvegicus (Rat)',\n",
    "    7227: 'Drosophila melanogaster (Fruit fly)',\n",
    "    6239: 'Caenorhabditis elegans (Nematode)',\n",
    "    3702: 'Arabidopsis thaliana (Plant)',\n",
    "    559292: 'Saccharomyces cerevisiae (Yeast)',\n",
    "    83333: 'Escherichia coli K-12',\n",
    "    284812: 'Schizosaccharomyces pombe (Fission yeast)'\n",
    "}\n",
    "\n",
    "for i, (taxon, count) in enumerate(top_species.items(), 1):\n",
    "    name = model_organisms.get(taxon, f'TaxonID {taxon}')\n",
    "    pct = count / len(train_tax) * 100\n",
    "    print(f\"   {i:2d}. {name:40s}: {count:6,} ({pct:5.2f}%)\")\n",
    "\n",
    "# Visualization\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    y=[model_organisms.get(t, f'Taxon {t}') for t in top_species.index],\n",
    "    x=top_species.values,\n",
    "    orientation='h',\n",
    "    marker_color='lightseagreen',\n",
    "    text=top_species.values,\n",
    "    textposition='outside'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Top 15 Species in Training Data',\n",
    "    xaxis_title='Number of Proteins',\n",
    "    yaxis_title='Species',\n",
    "    height=600,\n",
    "    yaxis={'categoryorder':'total ascending'}\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Distribution analysis\n",
    "print(\"\\nDistribution Analysis:\")\n",
    "top_10_pct = species_counts.head(10).sum() / len(train_tax) * 100\n",
    "top_50_pct = species_counts.head(50).sum() / len(train_tax) * 100\n",
    "print(f\"   Top 10 species cover:     {top_10_pct:.1f}% of proteins\")\n",
    "print(f\"   Top 50 species cover:     {top_50_pct:.1f}% of proteins\")\n",
    "print(f\"   Species with 1 protein:   {(species_counts == 1).sum():,}\")\n",
    "print(f\"   Species with <10 proteins:{(species_counts < 10).sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(135deg, #fa709a 0%, #fee140 100%); padding: 25px; border-radius: 12px; margin: 25px 0; box-shadow: 0 6px 15px rgba(0,0,0,0.2);\">\n",
    "  <h1 style=\"color: white; margin: 0; font-size: 32px; text-shadow: 2px 2px 4px rgba(0,0,0,0.3);\">\n",
    "    Term Co-occurrence Analysis\n",
    "  </h1>\n",
    "  <p style=\"color: rgba(255,255,255,0.9); margin: 10px 0 0 0; font-size: 16px;\">\n",
    "    Discovering functional relationships between GO terms\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Co-occurrence patterns reveal:**\n",
    "- Terms that commonly appear together\n",
    "- Potential functional modules\n",
    "- Cross-ontology relationships\n",
    "\n",
    "This information can help:\n",
    "- Feature engineering (term combinations)\n",
    "- Model architecture (multi-task learning)\n",
    "- Post-processing (co-prediction rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T14:21:36.548789Z",
     "iopub.status.busy": "2025-12-14T14:21:36.548379Z",
     "iopub.status.idle": "2025-12-14T14:21:39.516126Z",
     "shell.execute_reply": "2025-12-14T14:21:39.515120Z",
     "shell.execute_reply.started": "2025-12-14T14:21:36.548758Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Sample proteins for co-occurrence analysis\n",
    "sample_size = min(10000, train_terms['EntryID'].nunique())\n",
    "sample_proteins = train_terms['EntryID'].unique()[:sample_size]\n",
    "sample_data = train_terms[train_terms['EntryID'].isin(sample_proteins)]\n",
    "\n",
    "# Calculate co-occurrence\n",
    "print(\"Calculating term co-occurrence patterns...\")\n",
    "co_occurrence = Counter()\n",
    "\n",
    "for protein, group in tqdm(sample_data.groupby('EntryID'), desc=\"Processing\"):\n",
    "    terms = group['term'].tolist()\n",
    "    if len(terms) > 1:\n",
    "        # Count all pairs\n",
    "        for i in range(len(terms)):\n",
    "            for j in range(i+1, len(terms)):\n",
    "                pair = tuple(sorted([terms[i], terms[j]]))\n",
    "                co_occurrence[pair] += 1\n",
    "\n",
    "# Top co-occurring pairs\n",
    "top_pairs = sorted(co_occurrence.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "\n",
    "print(\"\\nTop 20 Co-occurring Term Pairs:\")\n",
    "print(f\"{'Rank':<6}{'Count':<8}{'Term 1':<15}{'Term 2':<15}{'Names'}\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for i, (pair, count) in enumerate(top_pairs, 1):\n",
    "    term1, term2 = pair\n",
    "    name1 = go_graph.nodes.get(term1, {}).get('name', 'Unknown')[:20]\n",
    "    name2 = go_graph.nodes.get(term2, {}).get('name', 'Unknown')[:20]\n",
    "    print(f\"{i:<6}{count:<8}{term1:<15}{term2:<15}{name1} + {name2}\")\n",
    "\n",
    "# Cross-ontology co-occurrence\n",
    "print(\"\\nCross-Ontology Co-occurrence:\")\n",
    "cross_onto = defaultdict(int)\n",
    "\n",
    "for protein, group in sample_data.groupby('EntryID'):\n",
    "    aspects = set(group['aspect'].tolist())\n",
    "    if len(aspects) > 1:\n",
    "        for asp in aspects:\n",
    "            cross_onto[asp] += 1\n",
    "\n",
    "total_proteins = len(sample_data['EntryID'].unique())\n",
    "print(f\"   Proteins with MF+BP+CC: {len(sample_data.groupby('EntryID')['aspect'].apply(lambda x: len(set(x)) == 3))}\")\n",
    "print(f\"   Proteins with 2 aspects: {len(sample_data.groupby('EntryID')['aspect'].apply(lambda x: len(set(x)) == 2))}\")\n",
    "print(f\"   Proteins with 1 aspect:  {len(sample_data.groupby('EntryID')['aspect'].apply(lambda x: len(set(x)) == 1))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); padding: 25px; border-radius: 12px; margin: 25px 0; box-shadow: 0 6px 15px rgba(0,0,0,0.2);\">\n",
    "  <h1 style=\"color: white; margin: 0; font-size: 32px; text-shadow: 2px 2px 4px rgba(0,0,0,0.3);\">\n",
    "    Train-Test Analysis\n",
    "  </h1>\n",
    "  <p style=\"color: rgba(255,255,255,0.9); margin: 10px 0 0 0; font-size: 16px;\">\n",
    "    Comparing distributions and identifying potential shifts\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: This is a **prospective evaluation**\n",
    "- Test proteins currently have NO annotations\n",
    "- During evaluation, only proteins that gain experimental validation will be scored\n",
    "- We can still analyze distributional properties\n",
    "\n",
    "**Key questions:**\n",
    "- How do test sequences compare to training sequences?\n",
    "- Is there protein ID overlap?\n",
    "- Are sequence length distributions similar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T14:21:39.517527Z",
     "iopub.status.busy": "2025-12-14T14:21:39.517161Z",
     "iopub.status.idle": "2025-12-14T14:21:41.777227Z",
     "shell.execute_reply": "2025-12-14T14:21:41.773957Z",
     "shell.execute_reply.started": "2025-12-14T14:21:39.517500Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load test sequences\n",
    "print(\"Loading test sequences...\")\n",
    "test_seqs = []\n",
    "for record in tqdm(SeqIO.parse(paths['test_seq'], 'fasta'), desc=\"Loading test set\"):\n",
    "    entry_id = record.id.split('|')[1] if '|' in record.id else record.id\n",
    "    test_seqs.append({\n",
    "        'EntryID': entry_id,\n",
    "        'length': len(record.seq)\n",
    "    })\n",
    "\n",
    "test_df = pd.DataFrame(test_seqs)\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAIN-TEST COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Training proteins:        {len(train_seqs):,}\")\n",
    "print(f\"Test proteins:            {len(test_df):,}\")\n",
    "print(f\"Test/Train ratio:         {len(test_df)/len(train_seqs):.2f}x\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Protein ID overlap\n",
    "train_ids = set(train_seqs['EntryID'])\n",
    "test_ids = set(test_df['EntryID'])\n",
    "overlap = train_ids.intersection(test_ids)\n",
    "\n",
    "print(f\"\\nProtein ID Overlap:\")\n",
    "print(f\"   Overlapping proteins:     {len(overlap):,}\")\n",
    "print(f\"   % of train in test:       {len(overlap)/len(train_ids)*100:.2f}%\")\n",
    "print(f\"   % of test in train:       {len(overlap)/len(test_ids)*100:.2f}%\")\n",
    "print(f\"   New proteins in test:     {len(test_ids - train_ids):,}\")\n",
    "\n",
    "# Sequence length comparison\n",
    "print(f\"\\nSequence Length Comparison:\")\n",
    "print(f\"{'Metric':<20}{'Train':<15}{'Test':<15}{'Difference'}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Mean':<20}{train_seqs['length'].mean():<15.1f}{test_df['length'].mean():<15.1f}{test_df['length'].mean() - train_seqs['length'].mean():.1f}\")\n",
    "print(f\"{'Median':<20}{train_seqs['length'].median():<15.0f}{test_df['length'].median():<15.0f}{test_df['length'].median() - train_seqs['length'].median():.0f}\")\n",
    "print(f\"{'Std Dev':<20}{train_seqs['length'].std():<15.1f}{test_df['length'].std():<15.1f}{test_df['length'].std() - train_seqs['length'].std():.1f}\")\n",
    "print(f\"{'Min':<20}{train_seqs['length'].min():<15}{test_df['length'].min():<15}{test_df['length'].min() - train_seqs['length'].min()}\")\n",
    "print(f\"{'Max':<20}{train_seqs['length'].max():<15,}{test_df['length'].max():<15,}{test_df['length'].max() - train_seqs['length'].max():,}\")\n",
    "\n",
    "# Visualization\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=train_seqs['length'],\n",
    "    name='Train',\n",
    "    opacity=0.7,\n",
    "    marker_color='lightblue',\n",
    "    nbinsx=100\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=test_df['length'],\n",
    "    name='Test',\n",
    "    opacity=0.7,\n",
    "    marker_color='lightcoral',\n",
    "    nbinsx=100\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Sequence Length Distribution: Train vs Test',\n",
    "    xaxis_title='Sequence Length',\n",
    "    yaxis_title='Count',\n",
    "    barmode='overlay',\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nGood news: Distributions are very similar!\")\n",
    "print(\"   No severe distribution shift\")\n",
    "print(\"   Model should generalize well\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14875579,
     "sourceId": 116062,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
